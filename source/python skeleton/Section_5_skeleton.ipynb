{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxpy as cp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "class color:\n",
    "    PURPLE = '\\033[95m'\n",
    "    CYAN = '\\033[96m'\n",
    "    DARKCYAN = '\\033[36m'\n",
    "    BLUE = '\\033[94m'\n",
    "    GREEN = '\\033[92m'\n",
    "    YELLOW = '\\033[93m'\n",
    "    RED = '\\033[91m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "    END = '\\033[0m'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the mean absolute error:\n",
    "$$\n",
    "MAE = \\frac{1}{N}\\sum_{i=1}^N |y_i-x_i^\\top\\theta|\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def get_MAE(theta, X, y):\n",
    "    ypred = X@theta.T\n",
    "    mae = np.average(np.abs(ypred - y), axis=0)\n",
    "    \n",
    "    # compare to MAE from sklearn:\n",
    "    mae_sklr = mean_absolute_error(X@theta.T, y)\n",
    "    assert(math.isclose(mae, mae_sklr))\n",
    "    return mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7.2\n",
    "Implement the problem from Question 7.1 (Use the `GLPK` solver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to solve LPs from previous question \n",
    "def solve_LP(X, Y, lambda_, k):\n",
    "    d = X.shape[1]\n",
    "    N = X.shape[0]\n",
    "\n",
    "    # auxiliary variables:\n",
    "    Beta = cp.Variable((N, 1))\n",
    "    b = cp.Variable((d, 1))\n",
    "    alpha = cp.Variable((1,1))\n",
    "\n",
    "    # variables to solve:\n",
    "    theta = cp.Variable((1, d))\n",
    "\n",
    "    # linear program:\n",
    "    prob = cp.Problem( cp.Minimize(k * alpha + cp.sum(Beta) + lambda_ * cp.sum(b)), \n",
    "                      [\n",
    "        alpha + Beta >= 1/k * (Y - X @ theta.T),\n",
    "        alpha + Beta >= -1/k * (Y - X @ theta.T),\n",
    "        Beta >= 0,\n",
    "        -b <= theta,\n",
    "        theta <= b\n",
    "    ])\n",
    "\n",
    "    # solve LP:\n",
    "    prob.solve(solver=cp.GLPK)\n",
    "    theta_opt = theta.value\n",
    "    alpha_opt = b.value\n",
    "    \n",
    "    opt_value = prob.value\n",
    "    dual_value = prob.constraints[0].dual_value\n",
    "    return theta_opt,  opt_value, dual_value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.00000000e-05, 1.20679264e-05, 1.45634848e-05, 1.75751062e-05,\n",
       "       2.12095089e-05, 2.55954792e-05, 3.08884360e-05, 3.72759372e-05,\n",
       "       4.49843267e-05, 5.42867544e-05, 6.55128557e-05, 7.90604321e-05,\n",
       "       9.54095476e-05, 1.15139540e-04, 1.38949549e-04, 1.67683294e-04,\n",
       "       2.02358965e-04, 2.44205309e-04, 2.94705170e-04, 3.55648031e-04,\n",
       "       4.29193426e-04, 5.17947468e-04, 6.25055193e-04, 7.54312006e-04,\n",
       "       9.10298178e-04, 1.09854114e-03, 1.32571137e-03, 1.59985872e-03,\n",
       "       1.93069773e-03, 2.32995181e-03, 2.81176870e-03, 3.39322177e-03,\n",
       "       4.09491506e-03, 4.94171336e-03, 5.96362332e-03, 7.19685673e-03,\n",
       "       8.68511374e-03, 1.04811313e-02, 1.26485522e-02, 1.52641797e-02,\n",
       "       1.84206997e-02, 2.22299648e-02, 2.68269580e-02, 3.23745754e-02,\n",
       "       3.90693994e-02, 4.71486636e-02, 5.68986603e-02, 6.86648845e-02,\n",
       "       8.28642773e-02, 1.00000000e-01])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyperparameters:\n",
    "lambda_ = np.logspace(-5, -1, 50, base = 10)\n",
    "lambda_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "diabetes = load_diabetes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 75% split:\n",
    "X, X_test, Y, Y_test = train_test_split(diabetes['data'], \n",
    "                                        np.expand_dims(diabetes['target'], 1), \n",
    "                                        test_size=0.25, random_state=0)\n",
    "# Add bias column to data:\n",
    "X = np.concatenate((X, np.ones((X.shape[0], 1))), axis=1)\n",
    "X_test = np.concatenate((X_test, np.ones((X_test.shape[0], 1))), axis=1)\n",
    "\n",
    "e_train, e_val, thetas, opt_val = [], [], [], []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Optimal values--\n",
      "Optimal lambda: 0.004094915062380423\n",
      "Optimal value: 69.85032423411252\n",
      "Optimal theta:\n",
      " [[ -51.25036737 -256.59298168  256.59298168  256.59298168  134.15685618\n",
      "  -256.59298168 -256.59298168  256.59298168  256.59298168  249.77586814\n",
      "   151.12806892]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Cross-validation for 50 values of lambda:\n",
    "for l in lambda_:\n",
    "    theta_opt, opt_value, dual_value = solve_LP(X, Y, l, math.floor(0.75 * X.shape[0]))\n",
    "    thetas.append(theta_opt)\n",
    "    opt_val.append(opt_value)\n",
    "    \n",
    "    # evaluate on training set:\n",
    "    e_train.append(get_MAE(theta_opt, X, Y))\n",
    "    # evaluate on validation set:\n",
    "    e_val.append(get_MAE(theta_opt, X_test, Y_test))\n",
    "    \n",
    "# take hyperparameter with smallest validation error:\n",
    "best_lambda = lambda_[np.argmin(e_val)]\n",
    "best_theta = thetas[np.argmin(e_val)]\n",
    "best_value = opt_val[np.argmin(e_val)]\n",
    "\n",
    "print('---Optimal values--')\n",
    "print(f'Optimal lambda: {best_lambda}')\n",
    "print(f'Optimal value: {best_value}')\n",
    "print(f'Optimal theta:\\n {best_theta}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mTraining Results\u001b[0m\n",
      "MAE: [46.42683699]\n",
      "\n",
      "\n",
      "\u001b[1mTest Results\u001b[0m\n",
      "MAE: [43.17935492]\n"
     ]
    }
   ],
   "source": [
    "print(color.BOLD + 'Training Results' + color.END)\n",
    "print('MAE: {}'.format(get_MAE(best_theta, X, Y)))\n",
    "print('\\n')\n",
    "print(color.BOLD + 'Test Results' + color.END)\n",
    "print('MAE: {}'.format(get_MAE(best_theta, X_test, Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
