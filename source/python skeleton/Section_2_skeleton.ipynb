{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxpy as cp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "class color:\n",
    "    PURPLE = '\\033[95m'\n",
    "    CYAN = '\\033[96m'\n",
    "    DARKCYAN = '\\033[36m'\n",
    "    BLUE = '\\033[94m'\n",
    "    GREEN = '\\033[92m'\n",
    "    YELLOW = '\\033[93m'\n",
    "    RED = '\\033[91m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "    END = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('To use color, just append color.BOLD to the beginning of the printed string and color.END to the end:')\n",
    "print(color.BOLD + 'Like This!' + color.END)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the mean absolute error:\n",
    "$$\n",
    "MAE = \\frac{1}{N}\\sum_{i=1}^N |y_i-x_i^\\top\\theta|\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def get_MAE(theta, X, y):\n",
    "    ypred = X@theta.T\n",
    "    mae = np.average(np.abs(ypred - y), axis=0)\n",
    "    \n",
    "    # compare to MAE from sklearn:\n",
    "    mae_sklr = mean_absolute_error(X@theta.T, y)\n",
    "    assert(math.isclose(mae, mae_sklr))\n",
    "    return mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "diabetes = load_diabetes()\n",
    "X, X_test, Y, Y_test = train_test_split(diabetes['data'],\n",
    "                                        np.expand_dims(diabetes['target'], 1),\n",
    "                                        test_size=0.5,\n",
    "                                        random_state=0)\n",
    "\n",
    "# Add bias column to data:\n",
    "X = np.concatenate((X, np.ones((X.shape[0], 1))), axis=1)\n",
    "X_test = np.concatenate((X_test, np.ones((X_test.shape[0], 1))), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(diabetes['DESCR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.2\n",
    "Implement below the mean-absolute error regression with LASSO. Use $\\lambda=0.5$. Hints: in the X matrix, rows represent data samples. Also, don't forget to add the `1` column to capture the intercept. (Use the `GLPK` solver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to solve LPs \n",
    "def solve_LP(X, Y, lambda_):\n",
    "    d = X.shape[1]\n",
    "    N = X.shape[0]\n",
    "\n",
    "    # auxiliary variables:\n",
    "    Z = cp.Variable((N, 1))\n",
    "    delta = cp.Variable((d, 1))\n",
    "\n",
    "    # variable to solve:\n",
    "    theta = cp.Variable((1, d))\n",
    "\n",
    "    # linear program:\n",
    "    prob = cp.Problem(cp.Minimize(cp.sum(Z) + lambda_ * cp.sum(delta)), [\n",
    "        Y - X @ theta.T <= Z, -Y + X @ theta.T <= Z, theta <= delta,\n",
    "        -theta <= delta\n",
    "    ])\n",
    "\n",
    "    # solve LP:\n",
    "    prob.solve()\n",
    "    theta_opt = theta.value\n",
    "    opt_value = prob.value\n",
    "    dual_value = prob.constraints[0].dual_value\n",
    "    return theta_opt, opt_value, dual_value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_ = 0.5\n",
    "theta_opt, opt_value, dual_value = solve_LP(X, Y, lambda_)\n",
    "\n",
    "# Print results:\n",
    "print(\"\\nThe optimal value is\", opt_value)\n",
    "print(\"A solution theta is\")\n",
    "print(theta_opt)\n",
    "print(f'Shape of solution:{theta_opt.shape}')\n",
    "\n",
    "#print(\"A dual solution is\")\n",
    "#print(dual_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(color.BOLD + 'Training Results' + color.END)\n",
    "print('MAE: {}'.format(get_MAE(theta_opt, X, Y)))\n",
    "print('\\n')\n",
    "print(color.BOLD + 'Test Results' + color.END)\n",
    "print('MAE: {}'.format(get_MAE(theta_opt, X_test, Y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2.3\n",
    "Implement Cross-Validation for your MAE LASSO regression. You may recycle any functions used above. Hint: Use the `sklearn` function `train_test_split`, which can be used to randomly split the data.\n",
    "\n",
    "Use cross-validation to tune the hyperparameter $\\lambda$. Randomly select 75% of the data to construct Dtrain and use the rest of the data to construct Dval. Use 50 logarithmically spaced values between [10e-5; 10e-1] as candidates for $\\lambda$, select the one performing best on the validation set in terms of MAE. Compare again the test performance against the training performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "10e-5, 10e-1, 1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameters:\n",
    "lambda_ = np.logspace(-5, -1, 50, base = 10)\n",
    "lambda_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 75% split:\n",
    "X, X_test, Y, Y_test = train_test_split(diabetes['data'], \n",
    "                                        np.expand_dims(diabetes['target'], 1), \n",
    "                                        test_size=0.25, random_state=0)\n",
    "# Add bias column to data:\n",
    "X = np.concatenate((X, np.ones((X.shape[0], 1))), axis=1)\n",
    "X_test = np.concatenate((X_test, np.ones((X_test.shape[0], 1))), axis=1)\n",
    "\n",
    "e_train, e_val, thetas, opt_val = [], [], [], []\n",
    "\n",
    "# Cross-validation for 50 values of lambda:\n",
    "for l in lambda_:\n",
    "    theta_opt, opt_value, dual_value = solve_LP(X, Y, l)\n",
    "    thetas.append(theta_opt)\n",
    "    opt_val.append(opt_value)\n",
    "    \n",
    "    # evaluate on training set:\n",
    "    e_train.append(get_MAE(theta_opt, X, Y))\n",
    "    # evaluate on validation set:\n",
    "    e_val.append(get_MAE(theta_opt, X_test, Y_test))\n",
    "    \n",
    "# take hyperparameter with smallest validation error:\n",
    "best_lambda = lambda_[np.argmin(e_val)]\n",
    "best_theta = thetas[np.argmin(e_val)]\n",
    "best_value = opt_val[np.argmin(e_val)]\n",
    "\n",
    "print('---Optimal values--')\n",
    "print(f'Optimal lambda: {best_lambda}')\n",
    "print(f'Optimal value: {best_value}')\n",
    "print(f'Optimal theta:\\n {best_theta}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(color.BOLD + 'Training Results' + color.END)\n",
    "print('MAE: {}'.format(get_MAE(best_theta, X, Y)))\n",
    "print('\\n')\n",
    "print(color.BOLD + 'Test Results' + color.END)\n",
    "print('MAE: {}'.format(get_MAE(best_theta, X_test, Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1)\n",
    "ax.plot(lambda_, e_val)\n",
    "ax.set_xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
