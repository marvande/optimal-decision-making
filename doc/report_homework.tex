\documentclass[10pt]{article}
\usepackage{hyperref}
\usepackage[T1]{fontenc}
\usepackage[latin1]{inputenc}
%\usepackage{beton}
%\usepackage{ccfonts}
%\usepackage{concrete}
\usepackage{concmath}
\usepackage{eulervm}
\usepackage{amsmath,amsthm,amssymb}
\usepackage{mathtools}
\usepackage{multicol}
\usepackage{marginnote}
\usepackage{pgfplots}

\usepackage{comment}


\usepackage{float}
\usepackage{hyperref}
\pgfplotsset{compat=1.5}
\usepackage{graphicx}

\graphicspath{ {./images/} }
\usepackage{listings}
\usepackage{xcolor}
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}

\usepackage{mathtools}

\usepackage{wasysym}
\usepackage[margin=1.5in]{geometry} 
\usepackage{enumerate}
\index{\usepackage}\usepackage{multicol}

\usepgfplotslibrary{fillbetween}


\newcommand{\N}{\mathbf{N}}
\newcommand{\Z}{\mathbb{Z}}

\newcommand{\R}{\mathbf{R}}
\newcommand{\C}{\mathbf{C}}
\newcommand{\Pbb}{\mathbb{P}}
\newcommand{\Fcal}{\mathcal{F}}
\newcommand{\Acal}{\mathcal{A}}
\newcommand{\Ecal}{\mathcal{E}}
\newcommand{\Ebb}{\mathbb{E}}
\newcommand{\Qbb}{\mathbb{Q}}

\newenvironment{theorem}[2][Theorem]{\begin{trivlist}
  \item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{lemma}[2][Lemma]{\begin{trivlist}
  \item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{exercise}[2][Exercise]{\begin{trivlist}
  \item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{reflection}[2][Reflection]{\begin{trivlist}
  \item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{proposition}[2][Proposition]{\begin{trivlist}
  \item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{corollary}[2][Corollary]{\begin{trivlist}
  \item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}

\newenvironment{definition}[2][Definition]{\begin{trivlist}
  \item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}

\begin{document}
	
  \renewcommand{\qedsymbol}{\smiley}
	\title{Optimal Decision Making \\ Report}
	\author{Aurelien Balice-Debbas, Marijn van der Meer, Hugo Bassas}
	
	\maketitle

\section{Mean Absolute Error Penalised Regression}

\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.5\textwidth]{doc/images/im1_.png}
  \caption{Figure 1: MSE vs. MAE estimators for a dataset with 1 outlier (red dot). }
  \vspace{-3mm}
  \label{fig:mae-mse}
\end{figure}

  \begin{exercise}{1} 
  (5 points).
  Figure~\ref{fig:mae-mse} visualizes the outputs predicted by the MSE
and MAE estimators on a dataset with 1 outlier. Explain the difference between the two estimators intuitively. Hint: Compare how Eq.~\ref{eq:mse} and Eq.~\ref{eq:mae} penalise errors. 

\begin{equation}\label{eq:mse}
    \hat\theta_{MSE} = arg min_\theta \frac{1}{N} \sum_{i=1}^N(y^{(i)}-\theta^Tx^{(i)})^2
\end{equation}

\begin{equation}\label{eq:mae}
    \hat\theta_{MAE} = arg min_\theta  \frac{1}{N} \sum_{i=1}^N|y^{(i)}-\theta^Tx^{(i)}|
\end{equation}


\textit{Answer.} The MSE estimator is computed from the sum of squared distances between our target variables $y^{(i)}$ and predicted values $\hat y^{(i)} = \theta^Tx^{(i)}$ while MAE uses the sum of absolute differences i.e., the average magnitudes of errors. MSE is more easy to use in optimisation problems because it is convex while MAE is not and has the same gradient everywhere. But MAE is more robust to outliers as can be seen in Fig.~\ref{fig:mae-mse}. This is due to the fact that MSE squares errors, and thus for an outlier we have that $e^2 >> |e|$. When searching for optimal weights $\hat\theta$, MSE will have a tendency to try to adjust in a way of minimising big errors such as the one outlier in our case. In our case, this explains why the model predicted using MSE (in blue) is closer to the outlier than the one from MAE (in red). 
  \end{exercise}

\begin{exercise}{2}
(5 points) In case of high-dimensional inputs (d > N), it makes sense to seek a sparse
parameter vector $\theta$. This means that many components of $\theta$ should be zero. The non-zero components of $\theta$ then correspond to the key features or key inputs that have a significant impact on the outputs. Sparsity can be induced by adding an L1-penalty to the objective function of Eq.~\ref{eq:mse} or Eq.~\ref{eq:mae}, which yields the least absolute
shrinkage and selection operator (LASSO) method of regression analysis. In case of the MAE objective, the resulting LASSO estimator satisfies:
\begin{equation}\label{eq:lasso}
    \hat\theta_{lasso} = arg min_\theta  \frac{1}{N} \sum_{i=1}^N|y^{(i)}-\theta^Tx^{(i)}| + \lambda ||\theta||_1 
\end{equation}
In practice, $\lambda$ is a hyperparameter typically chosen by cross validation. That is, we randomly partition D into a training dataset $D_{train}$
and a validation dataset $D_{val}$. We then solve Eq.~\ref{eq:lasso} using only $D_{train}$ for different
values of $\lambda$ and select the estimator that has minimal MAE on $D_{val}$. Rewrite Eq.~\ref{eq:lasso} as a linear program (not necessarily in standard form).

\textit{Answer.} 
We write $\Lambda = [\lambda_1, ... , \lambda_k]$ the set of all values we choose for the hyperparameter $\lambda$. We also note $x_i\in \mathbb{R}^d \; \forall i=1 ,\dots, N$. Then for $\lambda\geq 0$, LASSO regression finds $\theta \in \mathbb{R}^d$ such that   

\begin{equation}
\begin{array}{ll@{}lll}
\text{min} &\displaystyle\sum\limits_{i=1}^{N} & |y_i-\theta^Tx_i| + &\lambda\displaystyle\sum\limits_{i=1}^{d}|\theta_i|
\end{array}
\end{equation}

With the addition of auxiliary variables $z_i$ and $\delta_i$, we use the fact that $|y_i - \theta^T x_i| < z_i$ is equivalent to $-z_i < y_i - \theta^T x_i  < z_i$. Now minimising the sum of $z_i$ is equal to minimising the sum of $ |y_i-\theta^Tx_i|$ because $\sum\limits_{i=1}^{N}  |y_i-\theta^Tx_i| < \sum\limits_{i=1}^{N} z_i$. The same applies to $|\theta_i|$. So we can write the following linear program:
\begin{equation*}
\begin{array}{lll@{}lll}
&\text{min}  &\displaystyle\sum\limits_{i=1}^{N}z_i +  \lambda\sum\limits_{i=1}^{d}\delta_i &\\
\text{subject to} 
&& y_i-\theta^Tx_i &\leq z_i && i=1 ,\dots, N \\
&& -y_i+\theta^Tx_i &\leq z_i && i=1 ,\dots, N \\
                 && \theta_i &\leq \delta_i &&  i=1 ,\dots, d \\
                && -\theta_i &\leq \delta_i &&  i=1 ,\dots, d 
\end{array}
\end{equation*}
To rewrite this in standard form we introduce slack variables $s,t,u,v$: 
\begin{equation*}
\begin{array}{lll@{}lll}
&\text{min}  &\displaystyle\sum\limits_{i=1}^{N}z_i +  \lambda\sum\limits_{i=1}^{d}\delta_i &\\
\text{subject to} 
&& -\theta^Tx_i - z_i + s &= y_i && i=1 ,\dots, N \\
&& +\theta^Tx_i -z_i + t &= y_i && i=1 ,\dots, N \\
                 && \theta_i - \delta_i + u&= 0  &&  i=1 ,\dots, d \\
                && -\theta_i - \delta_i + v &= 0  &&  i=1 ,\dots, d 
                \\
                && s,t,u,v &\geq 0  && 
\end{array}
\end{equation*}



\end{exercise}

\begin{exercise}{3}

(5 points). Denote by $X$ the feasible set of problem (4). Write the
convex hull of $X$ using a finite set of simple (in-)equalities on $x_1$ and $x_2$ and find its vertices. Plot both $X$ and $conv(X)$.


\textit{Answer.}
Le $X$ ve the feasible set of problem (4), the convex hull can be written as : 


\begin{align*}
 x_1 + x_2 \leq 1 \\
 x_1  \geq 0 \\ 
 x_2 \geq 0 \\
 \end{align*}


The vertices are $(0, 0)$, $(0, 1)$ and $(1, 0 )$. 

Figure \ref{fig:hull} shows X and $Conv(X)$

\begin{figure}[!ht]
  \centering


\begin{tikzpicture}
\begin{axis}[
   % title={Temperature dependence of CuSO$_4\cdot$5H$_2$O solubility},
    xlabel={$x_1$},
    ylabel={$x_2$},
    xmin=-0.5, xmax=1.5,
    ymin=-0.5, ymax=1.5,
    xtick={0,0.5, 1, 1.5},
    ytick={0,0.5, 1, 1.5},
    legend pos=north west,
    ymajorgrids=true,
    grid style=dashed,
    axis lines = middle,
]


   
    
    \addplot [thick,color=blue,mark=o,fill=blue, 
                    fill opacity=0.05]coordinates {
            (0, 0) 
            (0, 1)
            (1, 0)  };
    
    
    \addplot[name path=f,domain=0:1,blue] {1 - x };
    %\addplot[name path=f,domain=-.15:1.05,blue] {};
    

    \path[name path=axis] (axis cs:0,0) -- (axis cs:1,0);
    
    \addplot [
        thick,
        color=blue,
        fill=blue, 
        fill opacity=0.05
    ]
    fill between[
        of=f and axis,
        soft clip={domain=0:1},
    ];
    
\end{axis}
\end{tikzpicture}

 \caption{Plot of $X$ and $Conv(X)$}
  \vspace{-3mm}
  \label{fig:hull}
\end{figure}

\end{exercise}

\begin{exercise}{4}
(5 points). Replace the feasible set of problem (4) with its convex hull and solve the resulting linear program using MATLAB or Python. Report the optimal decision variables. Explain why they must be binary. Hint:
Characterize the BFS of the convex hull of the feasible set.


\textit{Answer}
Using the convex hull, we get the following linear program : 


\begin{equation*}
\begin{array}{lll@{}lll}
&\text{min}  & - x_1 - 2 x_2  &\\
\text{subject to} 
&& x_1 + x_2 \leq 1 \\
&&  x_1  \geq 0,  x_2 \geq 0 \\
          
\end{array}
\end{equation*}

Solving the linear program with Python, we obtain  the following result : $x_1 =  0.0$ and $x_2 = 1.0$


%Let $x^{\*}$ be an extreme point %% Ore BSF, need to check 
%Suppose towards contradiction that there exist a BFS $x^*$ that is not binary. So that exists $x_i^*$ s.t $0< x_i^* < 1 $ 
%We want to define $y$ and $z$ so they are feasible solutions and $x^* = \frac{1}{2} (y + z) $ which contradicts the fact that $x^* $ is an extreme point. 
\end{exercise}
  
  

\begin{exercise}{5}
(20 points)
Part 1: Characterize your decisions variables and feasible set

\textit{Answer}
The only decision attributed to the player (the one who is trying to save his money) is how much money to put into each hiding spot $x_i$. The amount of money placed in each hiding spot must be subjected to two different constraints:

\begin{itemize}
    \item the amount of money in hiding spot \textit{i} cannot exceed the maximum amount of money that can be hidden in this spot $C_i$.
    \item the total amount of money hidden in all spots must be equal to the total amount of money the player has, i.e. must be equal to C.
\end{itemize}

To summarize, the constraints can be expressed as


Part 2:

\textit{Answer}

The two decision variables attributed to the adversary, in this case the robber (the one who is trying to take the money from the player) is how much time to put into searching for money, and where to spend this time. However, since these two variables are redundant, they can be condensed to one, which is the time spent searching for money, represented by the variable $z_i$. The time spent searching for money has two different constraints:

\begin{itemize}
    \item The total amount of time spent searching for money must not exceed the time \textit{T} needed for the police to arrive to the house.
    \item The time spent in one area multiplied by the area's search difficulty factor p cannot exceed 1. If it equals 1, it means all of the money in that spot has been found.
\end{itemize}

To summarized, the two constraints can be expressed as:

\end{exercise}

  
  
  \begin{exercise}{6}
  Write the convex hull of the adversary's feasible set using a finite set of (in-)equalities on $z_i$. Note that k is an integer. Explain why your formulation is indeed the convex hull.
  
  \textit{Answer :}
  
\begin{comment}


\begin{equation*}
\begin{array}{lll@{}lll}
\min\limits_{\theta}&\max\limits_{z_1,...,z_N}  &\frac{1}{k}\displaystyle\sum\limits_{i=1}^{N}z_i|y^{(i)} - \theta^\top x^{(i)}| +  \lambda||\theta||_1 &\\
\text{subject to} 
&& \displaystyle\sum\limits_{i=1}^{N}z_i = k  && \\
                \\
                && z_i \in  \{ 0, 1 \}   & \forall i \in [ N ]& 
\end{array}
\end{equation*}


Lets write the dual of the inner minimization problem : 

\begin{equation*}
\begin{array}{lll@{}lll}
&\max\limits_{z_1,...,z_N}  &\frac{1}{k}\displaystyle\sum\limits_{i=1}^{N}z_i|y^{(i)} - \theta^\top x^{(i)}|  &\\
\text{subject to} 
&& \displaystyle\sum\limits_{i=1}^{N}z_i = k  &&  \\
                \\
                && z_i \in  \{ 0, 1 \}   & \forall i \in [ N ]& 
\end{array}
\end{equation*}

We get : 

\begin{equation*}
\begin{array}{lll@{}lll}
&\min\limits_{\alpha}   & \alpha k   &\\
\text{subject to} 

&&  \alpha \geq |y^{(i)} - \theta^\top x^{(i)}|  &&  i=1 ,\dots, N \\
                \\
                && \alpha    & unconstrained& 
\end{array}
\end{equation*}

\end{comment}
  
  \end{exercise}

  \section*{Appendix}
\newpage  
 \begin{thebibliography}{9}

\end{thebibliography}
 
\end{document}

