\documentclass[10pt]{article}
\usepackage{hyperref}
\usepackage[T1]{fontenc}
\usepackage[latin1]{inputenc}
%\usepackage{beton}
%\usepackage{ccfonts}
%\usepackage{concrete}
\usepackage{concmath}
\usepackage{eulervm}
\usepackage{amsmath,amsthm,amssymb}
\usepackage{mathtools}
\usepackage{multicol}
\usepackage{marginnote}
\usepackage{pgfplots}
\setlength{\parindent}{0cm}

\usepackage{comment}


\usepackage{float}
\usepackage{hyperref}
\pgfplotsset{compat=1.5}
\usepackage{graphicx}

\graphicspath{ {./images/} }
\usepackage{listings}
\usepackage{xcolor}
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}

\usepackage{mathtools}

\usepackage{wasysym}
\usepackage[margin=1.5in]{geometry} 
\usepackage{enumerate}
\index{\usepackage}\usepackage{multicol}

\usepgfplotslibrary{fillbetween}


\newcommand{\N}{\mathbf{N}}
\newcommand{\Z}{\mathbb{Z}}

\newcommand{\R}{\mathbf{R}}
\newcommand{\C}{\mathbf{C}}
\newcommand{\Pbb}{\mathbb{P}}
\newcommand{\Fcal}{\mathcal{F}}
\newcommand{\Acal}{\mathcal{A}}
\newcommand{\Ecal}{\mathcal{E}}
\newcommand{\Ebb}{\mathbb{E}}
\newcommand{\Qbb}{\mathbb{Q}}

\newenvironment{theorem}[2][Theorem]{\begin{trivlist}
  \item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{lemma}[2][Lemma]{\begin{trivlist}
  \item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{exercise}[2][Exercise]{\begin{trivlist}
  \item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{reflection}[2][Reflection]{\begin{trivlist}
  \item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{proposition}[2][Proposition]{\begin{trivlist}
  \item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{corollary}[2][Corollary]{\begin{trivlist}
  \item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}

\newenvironment{definition}[2][Definition]{\begin{trivlist}
  \item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}

\begin{document}
	
  \renewcommand{\qedsymbol}{\smiley}
	\title{Optimal Decision Making \\ Report}
	\author{Aurelien Balice-Debbas, Marijn van der Meer, Hugo Bassas}
	
	\maketitle

\section{Mean Absolute Error Penalised Regression}

\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.5\textwidth]{doc/images/im1_.png}
  \caption{Figure 1: MSE vs. MAE estimators for a dataset with 1 outlier (red dot). }
  \vspace{-3mm}
  \label{fig:mae-mse}
\end{figure}

  \begin{exercise}{1} 
  (5 points).
  Figure~\ref{fig:mae-mse} visualizes the outputs predicted by the MSE
and MAE estimators on a dataset with 1 outlier. Explain the difference between the two estimators intuitively. Hint: Compare how Eq.~\ref{eq:mse} and Eq.~\ref{eq:mae} penalise errors. 

\begin{equation}\label{eq:mse}
    \hat\theta_{MSE} = arg min_\theta \frac{1}{N} \sum_{i=1}^N(y^{(i)}-\theta^Tx^{(i)})^2
\end{equation}

\begin{equation}\label{eq:mae}
    \hat\theta_{MAE} = arg min_\theta  \frac{1}{N} \sum_{i=1}^N|y^{(i)}-\theta^Tx^{(i)}|
\end{equation}


\textit{Answer.} The MSE estimator is computed from the sum of squared distances between our target variables $y^{(i)}$ and predicted values $\hat y^{(i)} = \theta^Tx^{(i)}$ while MAE uses the sum of absolute differences i.e., the average magnitudes of errors. MSE is more easy to use in optimisation problems because it is convex while MAE is not and has the same gradient everywhere. But MAE is more robust to outliers as can be seen in Fig.~\ref{fig:mae-mse}. This is due to the fact that MSE squares errors, and thus for an outlier we have that $e^2 >> |e|$. When searching for optimal weights $\hat\theta$, MSE will have a tendency to try to adjust in a way of minimising big errors such as the one outlier in our case. In our case, this explains why the model predicted using MSE (in blue) is closer to the outlier than the one from MAE (in red). 
  \end{exercise}

\begin{exercise}{2}
(5 points) In case of high-dimensional inputs (d > N), it makes sense to seek a sparse
parameter vector $\theta$. This means that many components of $\theta$ should be zero. The non-zero components of $\theta$ then correspond to the key features or key inputs that have a significant impact on the outputs. Sparsity can be induced by adding an L1-penalty to the objective function of Eq.~\ref{eq:mse} or Eq.~\ref{eq:mae}, which yields the least absolute
shrinkage and selection operator (LASSO) method of regression analysis. In case of the MAE objective, the resulting LASSO estimator satisfies:
\begin{equation}\label{eq:lasso}
    \hat\theta_{lasso} = arg min_\theta  \frac{1}{N} \sum_{i=1}^N|y^{(i)}-\theta^Tx^{(i)}| + \lambda ||\theta||_1 
\end{equation}
In practice, $\lambda$ is a hyperparameter typically chosen by cross validation. That is, we randomly partition D into a training dataset $D_{train}$
and a validation dataset $D_{val}$. We then solve Eq.~\ref{eq:lasso} using only $D_{train}$ for different
values of $\lambda$ and select the estimator that has minimal MAE on $D_{val}$. Rewrite Eq.~\ref{eq:lasso} as a linear program (not necessarily in standard form).

\textit{Answer.} 
We write $\Lambda = [\lambda_1, ... , \lambda_k]$ the set of all values we choose for the hyperparameter $\lambda$. We also note $x_i\in \mathbb{R}^d \; \forall i=1 ,\dots, N$. Then for $\lambda\geq 0$, LASSO regression finds $\theta \in \mathbb{R}^d$ such that   

\begin{equation}
\begin{array}{ll@{}lll}
\text{min} &\displaystyle\sum\limits_{i=1}^{N} & |y_i-\theta^Tx_i| + &\lambda\displaystyle\sum\limits_{i=1}^{d}|\theta_i|
\end{array}
\end{equation}

With the addition of auxiliary variables $z_i$ and $\delta_i$, we use the fact that $|y_i - \theta^T x_i| < z_i$ is equivalent to $-z_i < y_i - \theta^T x_i  < z_i$. Now minimising the sum of $z_i$ is equal to minimising the sum of $ |y_i-\theta^Tx_i|$ because $\sum\limits_{i=1}^{N}  |y_i-\theta^Tx_i| < \sum\limits_{i=1}^{N} z_i$. The same applies to $|\theta_i|$. So we can write the following linear program:
\begin{equation*}
\begin{array}{lll@{}lll}
&\text{min}  &\displaystyle\sum\limits_{i=1}^{N}z_i +  \lambda\sum\limits_{i=1}^{d}\delta_i &\\
\text{subject to} 
&& y_i-\theta^Tx_i &\leq z_i && i=1 ,\dots, N \\
&& -y_i+\theta^Tx_i &\leq z_i && i=1 ,\dots, N \\
                 && \theta_i &\leq \delta_i &&  i=1 ,\dots, d \\
                && -\theta_i &\leq \delta_i &&  i=1 ,\dots, d 
\end{array}
\end{equation*}
To rewrite this in standard form we introduce slack variables $s,t,u,v$: 
\begin{equation*}
\begin{array}{lll@{}lll}
&\text{min}  &\displaystyle\sum\limits_{i=1}^{N}z_i +  \lambda\sum\limits_{i=1}^{d}\delta_i &\\
\text{subject to} 
&& -\theta^Tx_i - z_i + s &= y_i && i=1 ,\dots, N \\
&& +\theta^Tx_i -z_i + t &= y_i && i=1 ,\dots, N \\
                 && \theta_i - \delta_i + u&= 0  &&  i=1 ,\dots, d \\
                && -\theta_i - \delta_i + v &= 0  &&  i=1 ,\dots, d 
                \\
                && s,t,u,v &\geq 0  && 
\end{array}
\end{equation*}



\end{exercise}

\section{Convex Hulls}
\begin{exercise}{3}

(5 points). Denote by $X$ the feasible set of problem (4). Write the
convex hull of $X$ using a finite set of simple (in-)equalities on $x_1$ and $x_2$ and find its vertices. Plot both $X$ and $conv(X)$.


\textit{Answer.}
Le $X$ ve the feasible set of problem (4), the convex hull can be written as : 


\begin{align*}
 x_1 + x_2 \leq 1 \\
 x_1  \geq 0 \\ 
 x_2 \geq 0 \\
 \end{align*}


The vertices are $(0, 0)$, $(0, 1)$ and $(1, 0 )$. 

Figure \ref{fig:hull} shows X and $Conv(X)$

\begin{figure}[!ht]
  \centering


\begin{tikzpicture}
\begin{axis}[
   % title={Temperature dependence of CuSO$_4\cdot$5H$_2$O solubility},
    xlabel={$x_1$},
    ylabel={$x_2$},
    xmin=-0.5, xmax=1.5,
    ymin=-0.5, ymax=1.5,
    xtick={0,0.5, 1, 1.5},
    ytick={0,0.5, 1, 1.5},
    legend pos=north west,
    ymajorgrids=true,
    grid style=dashed,
    axis lines = middle,
]


   
    
    \addplot [thick,color=blue,mark=o,fill=blue, 
                    fill opacity=0.05]coordinates {
            (0, 0) 
            (0, 1)
            (1, 0)  };
    
    
    \addplot[name path=f,domain=0:1,blue] {1 - x };
    %\addplot[name path=f,domain=-.15:1.05,blue] {};
    

    \path[name path=axis] (axis cs:0,0) -- (axis cs:1,0);
    
    \addplot [
        thick,
        color=blue,
        fill=blue, 
        fill opacity=0.05
    ]
    fill between[
        of=f and axis,
        soft clip={domain=0:1},
    ];
    
\end{axis}
\end{tikzpicture}

 \caption{Plot of $X$ and $Conv(X)$}
  \vspace{-3mm}
  \label{fig:hull}
\end{figure}

\end{exercise}

\begin{exercise}{4}
(5 points). Replace the feasible set of problem (4) with its convex hull and solve the resulting linear program using MATLAB or Python. Report the optimal decision variables. Explain why they must be binary. Hint:
Characterize the BFS of the convex hull of the feasible set.


\textit{Answer}
Using the convex hull, we get the following linear program : 


\begin{equation*}
\begin{array}{lll@{}lll}
&\text{min}  & - x_1 - 2 x_2  &\\
\text{subject to} 
&& x_1 + x_2 \leq 1 \\
&&  x_1  \geq 0,  x_2 \geq 0 \\
          
\end{array}
\end{equation*}

Solving the linear program with Python, we obtain  the following result : $x_1 =  0.0$ and $x_2 = 1.0$

I this case, the extreme points of $conv(D)$ have integer coordinates. MILP can be solved by finding an extreme point solution to CMILP

%Let $x^{\*}$ be an extreme point %% Ore BSF, need to check 
%Suppose towards contradiction that there exist a BFS $x^*$ that is not binary. So that exists $x_i^*$ s.t $0< x_i^* < 1 $ 
%We want to define $y$ and $z$ so they are feasible solutions and $x^* = \frac{1}{2} (y + z) $ which contradicts the fact that $x^* $ is an extreme point. 
\end{exercise}
  
  
 \section{Zero-Sum Games}
\begin{exercise}{5}
(20 points)
Part 1: Characterize your decisions variables and feasible set

\textit{Answer}
The only decision attributed to the player (the one who is trying to save his money) is how much money to put into each hiding spot $x_i$. The amount of money placed in each hiding spot must be subjected to two different constraints:

\begin{itemize}
    \item the amount of money in hiding spot \textit{i} cannot exceed the maximum amount of money that can be hidden in this spot $C_i$.
    \item the total amount of money hidden in all spots must be equal to the total amount of money the player has, i.e. must be equal to C.
\end{itemize}

To summarize, the constraints can be expressed as

\begin{equation}
\begin{array}{ll@{}lll}

\displaystyle\sum\limits_{i=1}^{I} x_i = C && i=1 ,\dots, I \\
&& x_i \leq C_i && i=1 ,\dots, I\\

\end{array}
\end{equation}




Part 2:

\textit{Answer}

The two decision variables attributed to the adversary, in this case the robber (the one who is trying to take the money from the player) is how much time to put into searching for money, and where to spend this time. However, since these two variables are redundant, they can be condensed to one, which is the time spent searching for money, represented by the variable $z_i$. The time spent searching for money has two different constraints:

\begin{itemize}
    \item The total amount of time spent searching for money must not exceed the time \textit{T} needed for the police to arrive to the house.
    \item The time spent in one area multiplied by the area's search difficulty factor p cannot exceed 1. If it equals 1, it means all of the money in that spot has been found.
\end{itemize}

To summarized, the two constraints can be expressed as:


Part 3:

\textit{Answer}
The problem can be written as the min-max problem below:

\begin{equation}
\begin{array}{ll@{}lll}
\text{min}_x\ \text{max}_z\ && \displaystyle\sum\limits_{i=1}^{I} x_i z_i p_i\\

\text{subject to} 
&& \displaystyle\sum\limits_{i=1}^{I} x_i = C\\
&& x_i \leq C_i && i=1 ,\dots, I\\
&& \displaystyle\sum\limits_{i=1}^{I} z_i \leq T \\
                && z_i p_i \leq 1  &&  i=1 ,\dots, I\\
                && x,z &\geq 0\\ 



\end{array}
\end{equation}


Part 4:

\textit{Answer}
Assuming that we keep the outer minimization constant while resolving the inner maximization problem, we can keep x constant. Thus, we can reformulate the maximization as follows:

\begin{equation}
\begin{array}{ll@{}lll}
\text{max}_y\ && \displaystyle\sum\limits_{i=1}^{I} x_i z_i p_i\\

\text{subject to} 
&& \displaystyle\sum\limits_{i=1}^{I} z_i \leq T &&  i=1 ,\dots, I \\
                && z_i p_i \leq 1  &&  i=1 ,\dots, I\\
                && z\ \geq 0\\ 

\end{array}
\end{equation}

The problem becomes a maximization problem centered on the variable z, which is the time allocated by the thief to searching each spot.
To write the dual problem, we take the dual variable y. The problem can be dualized as follows:

\begin{equation}
\begin{array}{ll@{}lll}
\text{min}_y\ &&Ty_{I+1} + \displaystyle\sum\limits_{i=1}^{I} \frac{1}{p_i} y_i\\

\text{subject to} 
&& y_i + y_{I+1} \geq x_i p_i  &&  i=1 ,\dots, I\\
&& y_i\ \geq 0 && i=1 ,\dots, I+1\\ 

\end{array}
\end{equation}

This problem finds the upper bound to the optimal time the thief can take.


\textit{Implement the resulting linear program in Python or MATLAB and solve it for the provided input data p;C 2 RI and T 2 R. Use the code skeletons available from Moodle. Report the worst amount of money you lose, the optimal plan for hiding the money and the thief 's optimal search strategy.}

Solving the problem in Python, we get :
\begin{itemize}
    \item   Worst amount of money we lose : 
    \item Optimal plan for hiding the money : 
    \item  Thief 's optimal search strategy : 
\end{itemize}

\end{exercise}

  
 \section{Adversarial Training}
  \begin{exercise}{6}
  Write the convex hull of the adversary's feasible set 
  \begin{equation}
      Z  =\Bigg \{ z\in \mathbb{R}^N: \sum_{i=1}^{N}z_i = k, z_i \in \{0,1\} \;\forall i\in[N]\Bigg \}
  \end{equation}
  
  using a finite set of (in-)equalities on $z_i$. Note that k is an integer. Explain why your formulation is indeed the convex hull.

  \textit{Answer:} First of all, we notice that all feasible points (and thus the convex hull) must be in the positive quadrant of $\mathbb{R}^N$ due to the coordinates of $z$ being $z_i \in \{0,1\}$. Furthermore, in the extreme case when $k=N$ then the only feasible point is $z_i = 1^N\in\mathbb{R}^N$ (the vector that is one everywhere). So, in the case of $k=N$, the convex hull is just the point itself (convex by definition). Another special case is when $k=1$ and the vectors $z$ are just the unit vectors $e_i \in\mathbb{R}^N$. In that case, the convex hull is given by the shape delimited by the unit vectors (a line in 2D and a triangle in 3D, Fig.~\ref{fig:convex-hull}). 
  
 \begin{figure}[!ht]
  \centering
  \includegraphics[width=0.5\textwidth]{doc/images/IMG_0038.jpg}
  \caption{Figure 1: Convex hull of feasible set in 2D and 3D in the case where $k=1$.}
  \vspace{-3mm}
  \label{fig:convex-hull}
\end{figure}
 
 For the more general case, we first note that, due to the combinatorial properties of the solutions $z\in\mathbb{R}^N$, the set of feasible solutions is finite i.e., because $z$ have to satisfy $\sum_{i=1}^{N}z_i = k$, there is only a finite number of possibilities assuming $N$ is finite. By Weyl's theorem~\cite{rockafellar1970convex}\cite{QUEYRANNE19921}, the convex hull of $Z$ is a convex polyhedron and can thus be defined by a finite set of linear inequalities.
 
 We define the convex hull by $conv(Z)= \{ z \in\mathbb{R}^N | Az\leq b, z_i \geq 0 \}$ where $A \in\mathbb{R}^{2xN} = \begin{pmatrix}
1 & 1 & ... & 1\\
-1 & 1 & ... & -1
\end{pmatrix}$ and $ b = \begin{pmatrix}k\\-k
\end{pmatrix}$. So, the vertices of the convex hull are the extreme points of the feasible set of the Integer Linear Problem ILP:

\begin{equation}
\begin{array}{l@{}llll}
&&\text{min}_\theta F(\theta, z)\\
\text{subject to} 
&& \displaystyle\sum_{i=1}^{N}z_i &\leq k & i=1 ,\dots, N \\
&& -\displaystyle\sum_{i=1}^{N}z_i &\leq -k & i=1 ,\dots, N  \\
&& z_i \in \{0,1\} && i=1 ,\dots, N 
\end{array}
\end{equation}
where $F(\theta, z) = max_{z_1, ...z_N} \displaystyle\frac{1}{k} \sum_{i=1}^{N}z_i|y^i-\theta^Tx^i|+\lambda || \theta||_1$. 
The convex hull as defined above has a finite number of constraints because its bounded by the number of constraints in the ILP, which is finite. The hull is convex by definition of the feasible region. 


Notes:
- linear integer program (LIP), an optimal integer solution is also optimal over the
convex hull of all integer feasible solutions,

 \end{exercise}
 
 
\begin{exercise}{7}

\begin{enumerate}
    \item 

We want to transform the following Min-Max problem. 

\begin{equation*}
\begin{array}{lll@{}lll}
\min\limits_{\theta}&\max\limits_{z_1,...,z_N}  &\frac{1}{k}\displaystyle\sum\limits_{i=1}^{N}z_i|y^{(i)} - \theta^\top x^{(i)}| +  \lambda||\theta||_1 &\\
\text{subject to} 
&& \displaystyle\sum\limits_{i=1}^{N}z_i = k  && \\
                \\
                && z_i \in  \{ 0, 1 \}   & \forall i \in [ N ]& 
\end{array}
\end{equation*}


First, using the solution from Question 3, lets write the dual of the inner minimization problem using the same logic : 


\begin{equation*}
\begin{array}{lll@{}lll}
&\max\limits_{z_1,...,z_N}  &\frac{1}{k}\displaystyle\sum\limits_{i=1}^{N}z_i|y^{(i)} - \theta^\top x^{(i)}| +  \lambda||\theta||_1 &\\
\text{subject to} 
&& \displaystyle\sum\limits_{i=1}^{N}z_i = k  &&  \\
                \\
                && z_i \in  \{ 0, 1 \}   & \forall i \in [ N ]& 
\end{array}
\end{equation*}

With $p_i$  replaced by $ 1 / k$ and $T$ replaced by $k$ making  $\beta_i$  unconstrained and changing the order of the constraint inequality because of the equality in the maximisation problem  . 

The maximization problem is transformed into : 

\begin{equation*}
\begin{array}{lll@{}lll}
&\min\limits_{y}   &  k\beta_{N + 1 }  + \sum\limits_{i=1}^{N}\beta_i  + \lambda||\theta||_1  &\\
\text{subject to} 

&&  \beta_{N + 1 } + \beta_i   & \geq \frac{1}{k}|y^{(i)} - \theta^\top x^{(i)}| &  i=1 ,\dots, N \\
                \\
                && y_i    &  \geq 0 % unconstrained& %% ?? Really unconstrained ? 
\end{array}
\end{equation*}

We then assigning $\beta_{N+1} = \alpha$, and as in Exercice 2, we use the fact that $|y_i - \theta^T x_i| \leq \beta_i$ is equivalent to $-\beta_i + \alpha < y_i - \theta^T x_i  < \beta_i + \alpha$ to rewrite the full linear program. 

We also use the same principle for $|\theta_i|$ using $b_i$ as an auxiliary variable. 

Finally, we obtain the following program : 

\begin{equation*}
\begin{array}{lll@{}lll}
 \min\limits_{\theta, b_i} &\min\limits_{\beta. \alpha}   &  k\alpha  + \sum\limits_{i=1}^{N}\beta_i  + \lambda\sum\limits_{i=1}^{d} b_i   &\\
\text{subject to} 

&&  \alpha + \beta_i    & \geq \frac{ y^{(i)} - \theta^\top x^{(i)}}{k}&  i=1 ,\dots, N \\
&&  \alpha + \beta_i    & \geq \frac{- y^{(i)} + \theta^\top x^{(i)}}{k} &  i=1 ,\dots, N \\
                \\
                && \beta_i    & \geq 0  & \\ %% ?? Really unconstrained ? 
                && - b_j \leq \theta_j \leq b_j & & j = 1 \dots, d
\end{array}
\end{equation*}

Merging the two minimization we obtain the desired result. 



\item \textit{Implement the linear program (8) in MATLAB or Python using the skeleton code we provide on Moodle and solve it for 50 logarithmically spaced values of in the interval. For each of these values compute the MAE of the corresponding estimator on the validation set, and select the best robust estimator. To assess the beneffts of robust cross validation over standard cross validation, compare the MAE of the robust estimator against the MEA of the estimator found in Question 2.3. In both cases the MEA should be computed on the test set.}




\item \textit{For the robust estimator found above, compare also the MEA on the test set against the MEA on the training set. Is there a significant difference to what you observed in Question 2.3.? Interpret your observations?}


We notice a significant difference between the MEA on the test set against the the Training Set. In this adversarial training procedure, we force the outliers to be in the training set.  Thus, it is logical that the MAE of the trainig set is larger than that of the test set. In the previous question, the outliers were evenly distributed between the training and the test, and the model was therefore less robust. 


\end{enumerate}

 \end{exercise}

  \section*{Appendix}
\newpage  
 \begin{thebibliography}{9}
\bibitem{rockafellar1970convex} 
Rockafellar, R Tyrrell: Convex analysis, 1970, volume 36, Princeton university press
\bibitem{QUEYRANNE19921} 
M. Queyranne and Y. Wang: On the convex hull of feasible solutions to certain combinatorial problems, 1992, volume 11, Operations Research Letters
\\\texttt{https://www.sciencedirect.com/science/article/pii/0167637792900558}
\end{thebibliography}
 
\end{document}

